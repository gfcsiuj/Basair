/**
 * useWitAiTracker - Custom Hook Ù„Ù„ØªØ³Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØªÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Wit.ai
 * 
 * ÙŠØ³Ø¬Ù„ ØµÙˆØª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø¨Ø± AudioContext (PCM Ù…Ø¨Ø§Ø´Ø±) ÙˆÙŠØ±Ø³Ù„ Ù…Ù‚Ø§Ø·Ø¹ WAV ÙƒÙ„ ~4 Ø«ÙˆØ§Ù†Ù
 * Ø¥Ù„Ù‰ Wit.ai API Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ Ø«Ù… ÙŠÙ‚Ø§Ø±Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©.
 */
import { useState, useRef, useCallback, useEffect } from 'react';
import { normalizeArabicText, fuzzyMatchWords } from '../utils/textUtils';

// ØªÙˆÙƒÙ† Wit.ai Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
const WIT_AI_TOKEN = import.meta.env.VITE_WIT_AI_TOKEN as string;

// Ù…Ø¯Ø© ÙƒÙ„ Ù…Ù‚Ø·Ø¹ ØµÙˆØªÙŠ Ø¨Ø§Ù„Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ© (4 Ø«ÙˆØ§Ù†Ù)
const CHUNK_INTERVAL_MS = 4000;

// Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
const SAMPLE_RATE = 16000;

interface UseWitAiTrackerOptions {
    expectedWords: string[];
    onWordMatch?: (index: number) => void;
    onWordMismatch?: (index: number) => void;
}

interface UseWitAiTrackerReturn {
    currentIndex: number;
    isListening: boolean;
    isLoading: boolean;
    start: () => Promise<void>;
    stop: () => void;
    resetIndex: () => void;
}

/**
 * ØªØ­ÙˆÙŠÙ„ Ø¹ÙŠÙ†Ø§Øª Float32 Ø¥Ù„Ù‰ Ù…Ù„Ù WAV (Blob)
 */
const encodeWAV = (samples: Float32Array, sampleRate: number): Blob => {
    const buffer = new ArrayBuffer(44 + samples.length * 2);
    const view = new DataView(buffer);

    const writeString = (offset: number, str: string) => {
        for (let i = 0; i < str.length; i++) {
            view.setUint8(offset + i, str.charCodeAt(i));
        }
    };

    writeString(0, 'RIFF');
    view.setUint32(4, 36 + samples.length * 2, true);
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, 1, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * 2, true);
    view.setUint16(32, 2, true);
    view.setUint16(34, 16, true);
    writeString(36, 'data');
    view.setUint32(40, samples.length * 2, true);

    let offset = 44;
    for (let i = 0; i < samples.length; i++, offset += 2) {
        const s = Math.max(-1, Math.min(1, samples[i]));
        view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
    }

    return new Blob([view], { type: 'audio/wav' });
};

/**
 * ØªÙ‚Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª (downsample)
 */
const downsample = (buffer: Float32Array, inputRate: number, outputRate: number): Float32Array => {
    if (inputRate === outputRate) return buffer;
    if (inputRate < outputRate) throw new Error('Input rate must be >= output rate');

    const ratio = inputRate / outputRate;
    const newLength = Math.round(buffer.length / ratio);
    const result = new Float32Array(newLength);

    for (let i = 0; i < newLength; i++) {
        const index = Math.round(i * ratio);
        result[i] = buffer[Math.min(index, buffer.length - 1)];
    }

    return result;
};

/**
 * Ø¥Ø±Ø³Ø§Ù„ Ù…Ù‚Ø·Ø¹ WAV Ø¥Ù„Ù‰ Wit.ai ÙˆØ§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªØ¹Ø±Ù Ø¹Ù„ÙŠÙ‡Ø§
 * ÙŠØ±Ø¬Ø¹ Ù…ØµÙÙˆÙØ© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª (tokens) Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù†Øµ ÙˆØ§Ø­Ø¯ Ù„Ø£Ù†Ù‡Ø§ Ø£Ø¯Ù‚
 */
const transcribeWithWitAi = async (wavBlob: Blob): Promise<string[]> => {
    const response = await fetch('https://api.wit.ai/speech?v=20240101', {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${WIT_AI_TOKEN}`,
            'Content-Type': 'audio/wav',
        },
        body: wavBlob,
    });

    if (!response.ok) {
        const errorBody = await response.text().catch(() => '');
        console.error('Wit.ai API error:', response.status, response.statusText, errorBody);
        return [];
    }

    const responseText = await response.text();
    const lines = responseText.trim().split('\n').filter(Boolean);

    let finalTokens: string[] = [];
    let finalText = '';

    for (const line of lines) {
        try {
            const parsed = JSON.parse(line);

            // Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ù…Ù† Ù…ØµÙÙˆÙØ© tokens (Ø£Ø¯Ù‚ Ù…Ù† Ø­Ù‚Ù„ text)
            if (parsed.speech?.tokens && Array.isArray(parsed.speech.tokens)) {
                finalTokens = parsed.speech.tokens
                    .map((t: any) => t.token as string)
                    .filter(Boolean);
            }

            if (parsed.text) {
                finalText = parsed.text;
            }
        } catch {
            // ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø·Ø± ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø©
        }
    }

    // Ø§Ø³ØªØ®Ø¯Ø§Ù… tokens Ø¥Ø°Ø§ Ù…ØªØ§Ø­Ø©ØŒ ÙˆØ¥Ù„Ø§ ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ
    const result = finalTokens.length > 0
        ? finalTokens
        : finalText.split(/\s+/).filter(Boolean);

    console.log('ğŸ“ Wit.ai recognized words:', result);
    return result;
};

export const useWitAiTracker = ({
    expectedWords,
    onWordMatch,
    onWordMismatch,
}: UseWitAiTrackerOptions): UseWitAiTrackerReturn => {
    const [currentIndex, setCurrentIndex] = useState(0);
    const [isListening, setIsListening] = useState(false);
    const [isLoading, setIsLoading] = useState(false);

    const currentIndexRef = useRef(0);
    const expectedWordsRef = useRef(expectedWords);
    const mediaStreamRef = useRef<MediaStream | null>(null);
    const audioContextRef = useRef<AudioContext | null>(null);
    const processorRef = useRef<ScriptProcessorNode | null>(null);
    const pcmBufferRef = useRef<Float32Array[]>([]);
    const chunkTimerRef = useRef<ReturnType<typeof setInterval> | null>(null);
    const isListeningRef = useRef(false);
    const isSendingRef = useRef(false);

    useEffect(() => {
        expectedWordsRef.current = expectedWords;
    }, [expectedWords]);

    useEffect(() => {
        currentIndexRef.current = currentIndex;
    }, [currentIndex]);

    /**
     * Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø³ØªÙ„Ù…Ø© Ù…Ù† Wit.ai ÙˆÙ…Ù‚Ø§Ø±Ù†ØªÙ‡Ø§ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
     */
    const processTranscription = useCallback((spokenWords: string[]) => {
        if (spokenWords.length === 0) return;

        const localIndex = currentIndexRef.current;
        const localExpected = expectedWordsRef.current;

        console.log('ğŸ¯ Processing:', { spokenWords, localIndex, totalExpected: localExpected.length });
        console.log('ğŸ¯ Next expected words:', localExpected.slice(localIndex, localIndex + 5));

        if (localIndex >= localExpected.length) {
            console.log('âœ… All words matched!');
            return;
        }

        let matchedCount = 0;

        for (const spokenWord of spokenWords) {
            const targetIndex = localIndex + matchedCount;
            if (targetIndex >= localExpected.length) break;

            const expectedWord = localExpected[targetIndex];
            // Ø­Ø¯ Ù…Ø·Ø§Ø¨Ù‚Ø© Ø£Ù‚Ù„ (60%) Ù„Ø£Ù† Wit.ai Ù„ÙŠØ³ Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ 100% Ù…Ø¹ Ø§Ù„Ù†Øµ Ø§Ù„Ù‚Ø±Ø¢Ù†ÙŠ
            const isMatch = fuzzyMatchWords(spokenWord, expectedWord, 0.6);

            console.log(`ğŸ”„ Comparing: "${spokenWord}" vs "${expectedWord}" => ${isMatch ? 'âœ… MATCH' : 'âŒ NO MATCH'}`);

            if (isMatch) {
                matchedCount++;
                onWordMatch?.(targetIndex);
            } else {
                // Ø¨Ø­Ø« ÙÙŠ Ù†Ø§ÙØ°Ø© Ø£ÙˆØ³Ø¹ Ù„Ù„Ø£Ù…Ø§Ù… (5 ÙƒÙ„Ù…Ø§Øª)
                let foundAhead = false;
                const searchWindow = Math.min(5, localExpected.length - targetIndex);
                for (let offset = 1; offset < searchWindow; offset++) {
                    if (fuzzyMatchWords(spokenWord, localExpected[targetIndex + offset], 0.6)) {
                        console.log(`ğŸ”„ Found ahead at offset ${offset}: "${localExpected[targetIndex + offset]}"`);
                        matchedCount += offset + 1;
                        foundAhead = true;
                        break;
                    }
                }

                if (!foundAhead) {
                    onWordMismatch?.(targetIndex);
                }
            }
        }

        console.log(`ğŸ“Š Total matched: ${matchedCount}`);
        if (matchedCount > 0) {
            const newIndex = localIndex + matchedCount;
            setCurrentIndex(newIndex);
        }
    }, [onWordMatch, onWordMismatch]);

    /**
     * Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ù…Ø¬Ù…Ù‘Ø¹ Ø¥Ù„Ù‰ Wit.ai
     */
    const sendChunkToWitAi = useCallback(async () => {
        if (pcmBufferRef.current.length === 0 || isSendingRef.current) return;

        const totalLength = pcmBufferRef.current.reduce((acc, buf) => acc + buf.length, 0);
        if (totalLength < 1000) return;

        const combined = new Float32Array(totalLength);
        let offset = 0;
        for (const buf of pcmBufferRef.current) {
            combined.set(buf, offset);
            offset += buf.length;
        }
        pcmBufferRef.current = [];

        isSendingRef.current = true;
        setIsLoading(true);
        try {
            const inputSampleRate = audioContextRef.current?.sampleRate || 44100;
            const downsampled = downsample(combined, inputSampleRate, SAMPLE_RATE);
            const wavBlob = encodeWAV(downsampled, SAMPLE_RATE);

            console.log(`ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ ${(wavBlob.size / 1024).toFixed(1)} KB WAV Ø¥Ù„Ù‰ Wit.ai`);

            const words = await transcribeWithWitAi(wavBlob);
            if (words.length > 0) {
                processTranscription(words);
            }
        } catch (error) {
            console.error('Error transcribing with Wit.ai:', error);
        } finally {
            isSendingRef.current = false;
            setIsLoading(false);
        }
    }, [processTranscription]);

    /**
     * Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ ÙˆØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¹Ø¨Ø± AudioContext (PCM Ù…Ø¨Ø§Ø´Ø±)
     */
    const start = useCallback(async () => {
        if (isListeningRef.current) return;

        if (!WIT_AI_TOKEN || WIT_AI_TOKEN === 'YOUR_WIT_AI_TOKEN_HERE') {
            console.error('Wit.ai token is not configured. Please set VITE_WIT_AI_TOKEN in .env.local');
            return;
        }

        try {
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                },
            });
            mediaStreamRef.current = stream;

            const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
            audioContextRef.current = audioContext;

            const source = audioContext.createMediaStreamSource(stream);
            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            processorRef.current = processor;

            processor.onaudioprocess = (e: AudioProcessingEvent) => {
                if (!isListeningRef.current) return;
                const inputData = e.inputBuffer.getChannelData(0);
                pcmBufferRef.current.push(new Float32Array(inputData));
            };

            source.connect(processor);
            processor.connect(audioContext.destination);

            chunkTimerRef.current = setInterval(() => {
                if (isListeningRef.current && pcmBufferRef.current.length > 0) {
                    sendChunkToWitAi();
                }
            }, CHUNK_INTERVAL_MS);

            isListeningRef.current = true;
            setIsListening(true);

            console.log('ğŸ¤ Ø¨Ø¯Ø£ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ø¹Ø¨Ø± Wit.ai (PCM Ù…Ø¨Ø§Ø´Ø±)');
        } catch (error) {
            console.error('Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†:', error);
            isListeningRef.current = false;
            setIsListening(false);
        }
    }, [sendChunkToWitAi]);

    /**
     * Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
     */
    const stop = useCallback(() => {
        if (!isListeningRef.current) return;

        isListeningRef.current = false;
        setIsListening(false);

        if (chunkTimerRef.current) {
            clearInterval(chunkTimerRef.current);
            chunkTimerRef.current = null;
        }

        if (pcmBufferRef.current.length > 0) {
            sendChunkToWitAi();
        }

        if (processorRef.current) {
            processorRef.current.disconnect();
            processorRef.current = null;
        }

        if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
            audioContextRef.current.close().catch(() => { });
            audioContextRef.current = null;
        }

        mediaStreamRef.current?.getTracks().forEach(track => track.stop());
        mediaStreamRef.current = null;

        pcmBufferRef.current = [];

        console.log('ğŸ”‡ ØªÙˆÙ‚Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹');
    }, [sendChunkToWitAi]);

    const resetIndex = useCallback(() => {
        setCurrentIndex(0);
        currentIndexRef.current = 0;
    }, []);

    useEffect(() => {
        return () => {
            stop();
        };
    }, [stop]);

    return {
        currentIndex,
        isListening,
        isLoading,
        start,
        stop,
        resetIndex,
    };
};
