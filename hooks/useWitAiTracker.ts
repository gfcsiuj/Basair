/**
 * useWitAiTracker - Custom Hook Ù„Ù„ØªØ³Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØªÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Wit.ai
 * 
 * ÙŠØ³Ø¬Ù„ ØµÙˆØª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø¨Ø± AudioContext (PCM Ù…Ø¨Ø§Ø´Ø±) ÙˆÙŠØ±Ø³Ù„ Ù…Ù‚Ø§Ø·Ø¹ WAV ÙƒÙ„ ~4 Ø«ÙˆØ§Ù†Ù
 * Ø¥Ù„Ù‰ Wit.ai API Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ Ø«Ù… ÙŠÙ‚Ø§Ø±Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©.
 */
import { useState, useRef, useCallback, useEffect } from 'react';
import { normalizeArabicText, fuzzyMatchWords } from '../utils/textUtils';

// ØªÙˆÙƒÙ† Wit.ai Ù…Ù† Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©
const WIT_AI_TOKEN = import.meta.env.VITE_WIT_AI_TOKEN as string;

// Ù…Ø¯Ø© ÙƒÙ„ Ù…Ù‚Ø·Ø¹ ØµÙˆØªÙŠ Ø¨Ø§Ù„Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ© (4 Ø«ÙˆØ§Ù†Ù)
const CHUNK_INTERVAL_MS = 4000;

// Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
const SAMPLE_RATE = 16000;

interface UseWitAiTrackerOptions {
    /** Ù…ØµÙÙˆÙØ© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© (Ù†Øµ Ø§Ù„Ø¢ÙŠØ© Ù…Ù‚Ø³Ù‘Ù… Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª) */
    expectedWords: string[];
    /** Ø¯Ø§Ù„Ø© ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ø¹Ù†Ø¯ ØªØ·Ø§Ø¨Ù‚ ÙƒÙ„Ù…Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) */
    onWordMatch?: (index: number) => void;
    /** Ø¯Ø§Ù„Ø© ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ø¹Ù†Ø¯ Ø¹Ø¯Ù… Ø§Ù„ØªØ·Ø§Ø¨Ù‚ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) */
    onWordMismatch?: (index: number) => void;
}

interface UseWitAiTrackerReturn {
    /** Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© */
    currentIndex: number;
    /** Ù‡Ù„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù†Ø´Ø·ØŸ */
    isListening: boolean;
    /** Ù‡Ù„ Ù†Ù†ØªØ¸Ø± Ø±Ø¯ Ù…Ù† Wit.aiØŸ */
    isLoading: boolean;
    /** Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ */
    start: () => Promise<void>;
    /** Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ */
    stop: () => void;
    /** Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„ÙÙ‡Ø±Ø³ */
    resetIndex: () => void;
}

/**
 * ØªØ­ÙˆÙŠÙ„ Ø¹ÙŠÙ†Ø§Øª Float32 Ø¥Ù„Ù‰ Ù…Ù„Ù WAV (Blob)
 */
const encodeWAV = (samples: Float32Array, sampleRate: number): Blob => {
    const buffer = new ArrayBuffer(44 + samples.length * 2);
    const view = new DataView(buffer);

    const writeString = (offset: number, str: string) => {
        for (let i = 0; i < str.length; i++) {
            view.setUint8(offset + i, str.charCodeAt(i));
        }
    };

    // RIFF header
    writeString(0, 'RIFF');
    view.setUint32(4, 36 + samples.length * 2, true);
    writeString(8, 'WAVE');

    // fmt sub-chunk
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true);       // sub-chunk size
    view.setUint16(20, 1, true);        // PCM format
    view.setUint16(22, 1, true);        // mono
    view.setUint32(24, sampleRate, true); // sample rate
    view.setUint32(28, sampleRate * 2, true); // byte rate
    view.setUint16(32, 2, true);        // block align
    view.setUint16(34, 16, true);       // bits per sample

    // data sub-chunk
    writeString(36, 'data');
    view.setUint32(40, samples.length * 2, true);

    // Write PCM samples
    let offset = 44;
    for (let i = 0; i < samples.length; i++, offset += 2) {
        const s = Math.max(-1, Math.min(1, samples[i]));
        view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
    }

    return new Blob([view], { type: 'audio/wav' });
};

/**
 * ØªÙ‚Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª (downsample) Ù…Ù† Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…ØµØ¯Ø± Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
 */
const downsample = (buffer: Float32Array, inputRate: number, outputRate: number): Float32Array => {
    if (inputRate === outputRate) return buffer;
    if (inputRate < outputRate) throw new Error('Input rate must be >= output rate');

    const ratio = inputRate / outputRate;
    const newLength = Math.round(buffer.length / ratio);
    const result = new Float32Array(newLength);

    for (let i = 0; i < newLength; i++) {
        const index = Math.round(i * ratio);
        result[i] = buffer[Math.min(index, buffer.length - 1)];
    }

    return result;
};

/**
 * Ø¥Ø±Ø³Ø§Ù„ Ù…Ù‚Ø·Ø¹ WAV Ø¥Ù„Ù‰ Wit.ai ÙˆØ§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ù†Øµ
 */
const transcribeWithWitAi = async (wavBlob: Blob): Promise<string> => {
    const response = await fetch('https://api.wit.ai/speech?v=20240101', {
        method: 'POST',
        headers: {
            'Authorization': `Bearer ${WIT_AI_TOKEN}`,
            'Content-Type': 'audio/wav',
        },
        body: wavBlob,
    });

    if (!response.ok) {
        const errorBody = await response.text().catch(() => '');
        console.error('Wit.ai API error:', response.status, response.statusText, errorBody);
        return '';
    }

    // Wit.ai /speech endpoint ÙŠØ±Ø¬Ø¹ Ø§Ø³ØªØ¬Ø§Ø¨Ø© NDJSON (Ø³Ø·ÙˆØ± JSON Ù…ØªØ¹Ø¯Ø¯Ø©)
    const responseText = await response.text();
    const lines = responseText.trim().split('\n').filter(Boolean);

    let finalText = '';
    for (const line of lines) {
        try {
            const parsed = JSON.parse(line);
            if (parsed.text) {
                finalText = parsed.text;
            }
        } catch {
            // ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø³Ø·Ø± ØºÙŠØ± Ø§Ù„ØµØ§Ù„Ø­Ø©
        }
    }

    return finalText;
};

export const useWitAiTracker = ({
    expectedWords,
    onWordMatch,
    onWordMismatch,
}: UseWitAiTrackerOptions): UseWitAiTrackerReturn => {
    const [currentIndex, setCurrentIndex] = useState(0);
    const [isListening, setIsListening] = useState(false);
    const [isLoading, setIsLoading] = useState(false);

    // refs Ù„ØªØ¬Ù†Ø¨ re-renders
    const currentIndexRef = useRef(0);
    const expectedWordsRef = useRef(expectedWords);
    const mediaStreamRef = useRef<MediaStream | null>(null);
    const audioContextRef = useRef<AudioContext | null>(null);
    const processorRef = useRef<ScriptProcessorNode | null>(null);
    const pcmBufferRef = useRef<Float32Array[]>([]);
    const chunkTimerRef = useRef<ReturnType<typeof setInterval> | null>(null);
    const isListeningRef = useRef(false);
    const isSendingRef = useRef(false);

    useEffect(() => {
        expectedWordsRef.current = expectedWords;
    }, [expectedWords]);

    useEffect(() => {
        currentIndexRef.current = currentIndex;
    }, [currentIndex]);

    /**
     * Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø³ØªÙ„Ù… Ù…Ù† Wit.ai ÙˆÙ…Ù‚Ø§Ø±Ù†ØªÙ‡ Ø¨Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
     */
    const processTranscription = useCallback((transcribedText: string) => {
        if (!transcribedText.trim()) return;

        const spokenWords = transcribedText.split(/\s+/).filter(Boolean);
        const localIndex = currentIndexRef.current;
        const localExpected = expectedWordsRef.current;

        if (localIndex >= localExpected.length) return;

        let matchedCount = 0;

        for (const spokenWord of spokenWords) {
            const targetIndex = localIndex + matchedCount;
            if (targetIndex >= localExpected.length) break;

            const expectedWord = localExpected[targetIndex];

            if (fuzzyMatchWords(spokenWord, expectedWord)) {
                matchedCount++;
                onWordMatch?.(targetIndex);
            } else {
                // Ø¨Ø­Ø« ÙÙŠ Ù†Ø§ÙØ°Ø© ØµØºÙŠØ±Ø© Ù„Ù„Ø£Ù…Ø§Ù…
                let foundAhead = false;
                const searchWindow = Math.min(3, localExpected.length - targetIndex);
                for (let offset = 1; offset < searchWindow; offset++) {
                    if (fuzzyMatchWords(spokenWord, localExpected[targetIndex + offset])) {
                        matchedCount += offset + 1;
                        foundAhead = true;
                        break;
                    }
                }

                if (!foundAhead) {
                    onWordMismatch?.(targetIndex);
                }
            }
        }

        if (matchedCount > 0) {
            const newIndex = localIndex + matchedCount;
            setCurrentIndex(newIndex);
        }
    }, [onWordMatch, onWordMismatch]);

    /**
     * Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‚Ø·Ø¹ Ø§Ù„ØµÙˆØªÙŠ Ø§Ù„Ù…Ø¬Ù…Ù‘Ø¹ Ø¥Ù„Ù‰ Wit.ai
     */
    const sendChunkToWitAi = useCallback(async () => {
        if (pcmBufferRef.current.length === 0 || isSendingRef.current) return;

        // ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ ÙÙŠ Ù…ØµÙÙˆÙØ© ÙˆØ§Ø­Ø¯Ø©
        const totalLength = pcmBufferRef.current.reduce((acc, buf) => acc + buf.length, 0);
        if (totalLength < 1000) return; // ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹

        const combined = new Float32Array(totalLength);
        let offset = 0;
        for (const buf of pcmBufferRef.current) {
            combined.set(buf, offset);
            offset += buf.length;
        }
        pcmBufferRef.current = [];

        isSendingRef.current = true;
        setIsLoading(true);
        try {
            // ØªÙ‚Ù„ÙŠÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¥Ù„Ù‰ 16000 Hz
            const inputSampleRate = audioContextRef.current?.sampleRate || 44100;
            const downsampled = downsample(combined, inputSampleRate, SAMPLE_RATE);
            const wavBlob = encodeWAV(downsampled, SAMPLE_RATE);

            console.log(`ğŸ“¤ Ø¥Ø±Ø³Ø§Ù„ ${(wavBlob.size / 1024).toFixed(1)} KB WAV Ø¥Ù„Ù‰ Wit.ai`);

            const text = await transcribeWithWitAi(wavBlob);
            if (text) {
                console.log('ğŸ“ Wit.ai:', text);
                processTranscription(text);
            }
        } catch (error) {
            console.error('Error transcribing with Wit.ai:', error);
        } finally {
            isSendingRef.current = false;
            setIsLoading(false);
        }
    }, [processTranscription]);

    /**
     * Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ ÙˆØªØ³Ø¬ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¹Ø¨Ø± AudioContext (PCM Ù…Ø¨Ø§Ø´Ø±)
     */
    const start = useCallback(async () => {
        if (isListeningRef.current) return;

        if (!WIT_AI_TOKEN || WIT_AI_TOKEN === 'YOUR_WIT_AI_TOKEN_HERE') {
            console.error('Wit.ai token is not configured. Please set VITE_WIT_AI_TOKEN in .env.local');
            return;
        }

        try {
            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    channelCount: 1,
                    echoCancellation: true,
                    noiseSuppression: true,
                },
            });
            mediaStreamRef.current = stream;

            // Ø¥Ù†Ø´Ø§Ø¡ AudioContext Ù„Ø§Ù„ØªÙ‚Ø§Ø· PCM Ù…Ø¨Ø§Ø´Ø±Ø©
            const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();
            audioContextRef.current = audioContext;

            const source = audioContext.createMediaStreamSource(stream);

            // ScriptProcessorNode Ù„Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„Ø®Ø§Ù…
            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            processorRef.current = processor;

            processor.onaudioprocess = (e: AudioProcessingEvent) => {
                if (!isListeningRef.current) return;
                const inputData = e.inputBuffer.getChannelData(0);
                // Ù†Ø³Ø® Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø£Ù†Ù‡Ø§ Ø³ØªØªØºÙŠØ±
                pcmBufferRef.current.push(new Float32Array(inputData));
            };

            source.connect(processor);
            processor.connect(audioContext.destination);

            // Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø¬Ù…Ù‘Ø¹Ø© ÙƒÙ„ CHUNK_INTERVAL_MS
            chunkTimerRef.current = setInterval(() => {
                if (isListeningRef.current && pcmBufferRef.current.length > 0) {
                    sendChunkToWitAi();
                }
            }, CHUNK_INTERVAL_MS);

            isListeningRef.current = true;
            setIsListening(true);

            console.log('ğŸ¤ Ø¨Ø¯Ø£ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ø¹Ø¨Ø± Wit.ai (PCM Ù…Ø¨Ø§Ø´Ø±)');
        } catch (error) {
            console.error('Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†:', error);
            isListeningRef.current = false;
            setIsListening(false);
        }
    }, [sendChunkToWitAi]);

    /**
     * Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯
     */
    const stop = useCallback(() => {
        if (!isListeningRef.current) return;

        isListeningRef.current = false;
        setIsListening(false);

        // Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¤Ù‚Øª
        if (chunkTimerRef.current) {
            clearInterval(chunkTimerRef.current);
            chunkTimerRef.current = null;
        }

        // Ø¥Ø±Ø³Ø§Ù„ Ø¢Ø®Ø± Ù…Ù‚Ø·Ø¹
        if (pcmBufferRef.current.length > 0) {
            sendChunkToWitAi();
        }

        // Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬
        if (processorRef.current) {
            processorRef.current.disconnect();
            processorRef.current = null;
        }

        // Ø¥ØºÙ„Ø§Ù‚ AudioContext
        if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
            audioContextRef.current.close().catch(() => { });
            audioContextRef.current = null;
        }

        // Ø¥ÙŠÙ‚Ø§Ù Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ©
        mediaStreamRef.current?.getTracks().forEach(track => track.stop());
        mediaStreamRef.current = null;

        // ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙØ±
        pcmBufferRef.current = [];

        console.log('ğŸ”‡ ØªÙˆÙ‚Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹');
    }, [sendChunkToWitAi]);

    /**
     * Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„ÙÙ‡Ø±Ø³
     */
    const resetIndex = useCallback(() => {
        setCurrentIndex(0);
        currentIndexRef.current = 0;
    }, []);

    // ØªÙ†Ø¸ÙŠÙ Ø¹Ù†Ø¯ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…ÙƒÙˆÙ†
    useEffect(() => {
        return () => {
            stop();
        };
    }, [stop]);

    return {
        currentIndex,
        isListening,
        isLoading,
        start,
        stop,
        resetIndex,
    };
};
